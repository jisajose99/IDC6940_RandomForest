##Maddie's Literature Reviews
## Week 2 Research Progress

[@rigatti2017random]
The goal of this paper is to explore and describe
the random forest technique. It provides an overview of the decision
tree process and how random forests are created using decision trees.
The explanations in the article are very useful for understanding the
process. It goes into detail about CART models, how a decision tree is
formed using randomization techniques such as bootstrapping and bagging,
and then randomization at each node. The article briefly explains being
able to tune the model with number of splits and number of trees, which
affect the computational intensity. The article then focuses on using a
random forest to analyze survival data. Colon cancer data is used to
create models using both random forest and Cox, and results are found to
be comparable. The main drawback the author writes of the random forest
as compared to Cox, is that it doesn't give insights into partial
effects of the predictors, making it much more difficult to understand
how the predictors may individually or collectively be influencing the
prediction.

[@oshiro2012many]
The purpose of this article is to research whether there is an optimal number of trees within a random forest. The goal is to determine if there's a threshold in which increasing the number of trees provides no significant performance gain as compared to the increase of computational cost. The article states that in general, the user sets the number of trees on a trial and error basis. They review the results from an analysis of increasing the number of trees, between 2 and 4096, doubling the number of trees at every iteration for 29 data sets. The number of attributes within each random forest is also analyzed with the growth of trees. Results were analyzed using ROC curve (AUC) and the percentage of attributes used. In general, as the number of trees increased, so did the AUC. However, there was no significant difference between the given number of trees and double that number. We needed to do at least 4x the number to see a significant improvement. There was no significant difference between 128 trees until 4096 trees. The article suggests that based on their experiments, a range between 64 and 128 trees in a forest tends to be an optimal balance between AUC, processing time, and memory usage. 

## Week 3 Research Progress

[@probst2019hyperparameters]
The purpose of this article is to examine the hyperparameters and tuning strategies for random forests. It reviews the various hyperparameters that must be set (i.e. number of trees, number of observations drawn randomly for each tree, replacement, number of variables for each split, etc). There are defaults for all of these hyperparameters if the user does not specify, however, the purpose of tuning these hyperparameters is to achieve optimal results for the specific data set being utilized. The article breaks down the hyperparameter analysis into sections – influence on performance and influence on variable importance; with subsection for each hyperparameter. They state that an optimal compromise between low correlation and reasonable strength of the trees has to be found, which can be controlled by the parameters mtry, sample size, and node size. The article did a great job diving into the different hyperparameters, explaining their purpose, why typical default values are what they are, and how choosing different values can affect the models. The article discusses how random forests are generally known to provide good results using the default settings, and tuning of hyperparameters is overall much less necessary/beneficial than other algorithms. It reviews various R packages and functions that can be used for hyperparameter tuning and how each function works – such as tuneRF which calculates the out-of-bag error with the default mtry value, and then tries smaller or larger values until there is no more improvement and returns the model with the best value. The authors also delve into the details of a tuning package they created, tuneRanger, and compare it to existing R packages. Overall, it exceeded in nearly every category – MMCE, AUC, Brier score, Logarithmic loss, and Training runtime. The variable mtry has shown to have the most potential when tuning a random forest model. 

[@dai2018using]
The purpose of this paper is to analyze the diagnosis of breast cancer using the random forest algorithm. The article begins with giving a basic background of the algorithm and how it functions. It then breaks down into sections further detailing the different steps and aspects of the random forest, such as bagging sampling and decision tree construction. “That is to say, in each round of random sampling of bagging, about 36.8% of the data in the training set is not collected by the sample set.” This is referred to as the Out Of Bag data, which is then used for testing the model. The data set used in the paper is the University of California, Irvine Breast Cancer Wisconsin (Diagnostic) Dataset, which has 569 medical records of multidimensional data. The article describes the different fields that are in the data set and does some basic proportional and correlation analysis. The author was able to achieve approximately 95% accuracy on the model and ROC OOB of 99.3%. This shows that the random forest can be a great choice in diagnosis prediction by producing classification results. 

## Week 4 Research Progress
[@khalilia2011predicting]
The purpose of this article is to explore predicting different diseases using random forest when the data set is highly imbalanced. “Class imbalance occurs if one class contains significantly more samples than the other class. Since the classification process assumes that the data is drawn from the same distribution as the training data, presenting imbalanced data to the classifier will produce undesirable results.” The data set being used is from the Nationwide Inpatient Sample (NIS) and has approximately 8 million records of hospital stays and 126 data elements for each stay. A potential hurdle that was mentioned in the article is that there is no patient identifier in the data set, therefore no way to determine if multiple records belong to the same patient or if time elapsed between diagnoses. The author describes the pre-processing that was necessary for the data set, such as parsing the data as it was not delimited. They describe how repeated random sub-sampling was effective for handling the highly imbalanced data set. In this, they partition the data set into sub-samples, each containing an equal number of instances from each class. “RF uses the Gini measure of impurity to select the split with the lowest impurity at every node.” The author discusses variable important in random forests, which they define as “Variable importance measures the degree of association between a given variable and the classification result. RF has four measures for the variable importance: raw importance score for class 0, raw importance score for class 1, decrease in accuracy and the Gini index”. For their model evaluation, they compared the results against SVM, boosting, and bagging; using ROC and AUC for the comparison. They found that on 7 of the 8 categories tested against, RF performed the best. They compared results using sampling and non-sampling and found that the classifications using sampling performed better in both accuracy and speed. The average RF AUC across all categories was 89.05%. 

[@boulesteix2012overview]
The purpose of this article is to provide an overview of the random forest algorithm, with a specific emphasis on applications to bioinformatics and computational biology. One item that was discussed was VIMs, or variable importance measures. They claim that “Random forest (RF) methodology is used to address two main classes of problems: to construct a prediction rule in a supervised learning problem and to assess and rank variables with respect to their ability to predict the response.” Variable importance measures, or VIMs, are automatically computed for each predictor in the random forest algorithm and can be used to identify predictors that are involved in interactions. Since random forest works well with high-dimensional data, it is often appropriate for clinical or bioinformatics use cases. The author describes the basics of how the random forest algorithm functions, and then brings up the standard method has an important pitfall: “In the split selection process, predictors may be favored or disfavored depending on their scale of measurement or, in the case of categorical predictors, on their number of categories.” The author further describes this pitfall in the article, and how it can lead to bias. The author describes another class of decision trees, called conditional inference forests (CIF), which addresses the issue found in the standard random forest. The author brings up how if there’s a large number of predictors, then the default value of sqrt(p) or p/3 may be too small, as in their case it happened to randomly select predictors that were all noninformative, resulting in trees that weren’t accurate. The article reviews the use cases of RF in genetic epidemiology, and using SNP’s (single nucleotide polymorphism)  as predictors, and shows an analysis using SNP data to demonstrate how the Gini VIM favored uncorrelated SNPs over strongly correlated SNPs, even if they were all noninformantive; therefore it was recommended to use permutation VIM. In conclusion, the RF algorithm is a useful tool for prediction, but does have its setbacks such as producing less than desirable results stemmed from bias or poor distribution of the predictor. 

##Jisa's Literature Reviews
## Week 2 Research Progress
Article 1:
Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5–32. https://doi.org/10.1023/A:1010933404324Links to an external site.

This foundational document presents the Random Forest algorithm, describing how it is formed by aggregating many decision trees. Breiman explains that each tree is built using a random selection of the features and the data. This randomness makes the ensemble model much more robust and accurate. The Random Forest is also able to work with large datasets that have a high dimensionality and has variable importance measures built right into it.

Article 2:
Biau, G., & Scornet, E. (2016). A random forest guided tour. Test, 25(2), 197–227. https://doi.org/10.1007/s11749-016-0481-7Links to an external site.

This article provides a detailed review of the Random Forest algorithm, with an emphasis on the  foundational theories and practical real world applications. The authors describe, in depth, the mathematics that drive the algorithm, covering aspects like:
 the selection of parameters that control algorithm performance
the resampling mechanism (i.e., making bootstrap samples of the training data) that makes the algorithm work
the measures of variable importance that help one understand how and why the algorithm reaches the conclusions it does.

## Week 3 Research Progress
Article 3: "Random Forests for Clinical Decision Support Systems" by Zhang et al. (2018)
Goal: To show how to apply Random Forests to predict patient sepsis risk in ICU patients
Why Important: Seizure can be predicted early and this helps to reduce mortality rates to improve our patient outcomes.
Methods:  Input features for the algorithm were patient vitals and lab results, feature importance in mind (e.g., lactate levels, BP worsening).
Results/Limitations: There was high sensitivity and specificity but the model needed constant updating for evolving ICU regimens.

Article 4: "Random Forest-Based Prediction of Diabetes Progression" by Smith et al. (2021)
Goal: Prediction progression of diabetes from longitudinal patient data
Why Important: Provides insight for tailoring interventions to delay or potentially avoiding disease progression in the hands of healthcare providers.
Methods: A patient health metrics dataset (eg, glucose values, BMI, age) was used to train the RF logistic model for cross-validation performance checks.
Results/Limitations: over 90% accuracy but the model was too complex so interpretability failed.

## Week 4 Research Progress
Article 5
Application of Random Forest Model to Predict the Demand of Essential Medicines for Non-Communicable Diseases Management in Public Health Facilities” by Mbonyinshuti et al. (2022)

Goal: To build a predictive model using the Random Forest approach to estimate the consumption for essential medicines used in managing NCDs in public health facilities in Rwanda.
Importance: Proper forecasting will guarantee the uninterrupted supply of essential medicines which in turn prevents healthcare shortages as well as optimizes the supply chain management within healthcare systems.
Methods: This study approached demand forecasting with the consumption data from 2015 to 2019 for around 500 medical products. To achieve this, first a Random Forest model was trained for demand prediction on a monthly basis using month, year, district and name of medicine as the main variables of the model. After that, the data was processed sequentially as the model’s name suggests.
Results/Limitations: The model achieved an accuracy of 78% for the training set and 71% for the testing set which is good enough to determine medicine demand trends and thus is useful in predicting medicine demand. However, the limitation of this study is that the data only covers Rwanda and the rest of the world has yet to see if this model is applicable for other regions.

 
Article 6
Random Forest Algorithms in Health Care Sectors: A Review of Applications” by Haripriya et al. (2021)
Goal: To evaluate the use of Random Forest algorithms in healthcare in terms of predicting patient safety outcomes and analyzing the components of safety culture.
Importance: Knowing the factors that affect patient safety is a very important consideration if the quality of healthcare and medical mistakes are to be reduced.
Methods: The study examined the records of several hospitals in the United States under the assumption that the organizational safety components are related to the grade the patient receives for safety in the hospital. They used Random Forest member classification in their assessments.
Results/Limitations: The analysis established that the knowledge pertaining the quality of health care, organizational factors, and the goals of top management have ascertained a significant impact on the patient safety outcome. This study highlights the importance of further research to make the conclusions more applicable across different types of healthcare systems.
